{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "\n",
    "In this assignment, you'll scrape text from [The California Aggie](https://theaggie.org/) and then analyze the text.\n",
    "\n",
    "The Aggie is organized by category into article lists. For example, there's a [Campus News](https://theaggie.org/campus/) list, [Arts & Culture](https://theaggie.org/arts/) list, and [Sports](https://theaggie.org/sports/) list. Notice that each list has multiple pages, with a maximum of 15 articles per page.\n",
    "\n",
    "The goal of exercises 1.1 - 1.3 is to scrape articles from the Aggie for analysis in exercise 1.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.1.__ Write a function that extracts all of the links to articles in an Aggie article list. The function should:\n",
    "\n",
    "* Have a parameter `url` for the URL of the article list.\n",
    "\n",
    "* Have a parameter `page` for the number of pages to fetch links from. The default should be `1`.\n",
    "\n",
    "* Return a list of aricle URLs (each URL should be a string).\n",
    "\n",
    "Test your function on 2-3 different categories to make sure it works.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* Be polite to The Aggie and save time by setting up [requests_cache](https://pypi.python.org/pypi/requests-cache) before you write your function.\n",
    "\n",
    "* Start by getting your function to work for just 1 page. Once that works, have your function call itself to get additional pages.\n",
    "\n",
    "* You can use [lxml.html](http://lxml.de/lxmlhtml.html) or [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to scrape HTML. Choose one and use it throughout the entire assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import corpus\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests_cache\n",
    "requests_cache.install_cache('demo_cache')\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud,STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def art_search(url, numpages):\n",
    "    '''\n",
    "    Augument: \n",
    "        Given the base URl and the page number to get all the url of article.\n",
    "        \n",
    "    Input:\n",
    "        URL and number of pages\n",
    "        \n",
    "    Output:\n",
    "        URL\n",
    "    \n",
    "    '''\n",
    "    b = []\n",
    "    for i in range(numpages):\n",
    "        urllist = url + '/page/'+ str(i+1) \n",
    "        response = requests.get(urllist)\n",
    "        html_doc = response.text\n",
    "        soup = BeautifulSoup(html_doc, 'lxml')\n",
    "        for j in soup.find_all('h2', class_ = \"entry-title\"):\n",
    "            b.append(j.a['href'])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://theaggie.org/2017/02/24/2017-winter-quarter-election-results/',\n",
       " 'https://theaggie.org/2017/02/23/university-of-california-davis-city-council-sever-wells-fargo-contracts/',\n",
       " 'https://theaggie.org/2017/02/23/academics-unite-in-peaceful-rally-against-immigration-ban/',\n",
       " 'https://theaggie.org/2017/02/23/memorial-union-to-reopen-spring-quarter/',\n",
       " 'https://theaggie.org/2017/02/23/asucd-president-alex-lee-vetoes-amendment-for-creation-of-judicial-council/',\n",
       " 'https://theaggie.org/2017/02/22/senate-candidate-zaki-shaheen-withdraws-from-race/',\n",
       " 'https://theaggie.org/2017/02/21/uc-davis-experiences-several-recent-hate-based-crimes/',\n",
       " 'https://theaggie.org/2017/02/21/uc-president-selects-gary-may-as-new-uc-davis-chancellor/',\n",
       " 'https://theaggie.org/2017/02/20/katehi-controversy-prompts-decline-of-uc-administrators-seeking-profitable-subsidiary-board-positions/',\n",
       " 'https://theaggie.org/2017/02/20/asucd-senate-passes-resolution-submitting-comments-on-lrdp/',\n",
       " 'https://theaggie.org/2017/02/20/uc-releases-2016-annual-report-on-sustainable-practices/',\n",
       " 'https://theaggie.org/2017/02/19/uc-davis-global-affairs-holds-discussion-on-president-donald-trumps-executive-orders-on-immigration/',\n",
       " 'https://theaggie.org/2017/02/19/trumps-immigration-ban-affects-uc-davis-community/',\n",
       " 'https://theaggie.org/2017/02/17/uc-davis-students-participate-in-uc-wide-nodapl-day-of-action/',\n",
       " 'https://theaggie.org/2017/02/17/uc-davis-holds-first-mental-health-conference/',\n",
       " 'https://theaggie.org/2017/02/16/last-week-in-senate-6/',\n",
       " 'https://theaggie.org/2017/02/16/2017-asucd-winter-elections-meet-the-candidates/',\n",
       " 'https://theaggie.org/2017/02/14/shields-library-hosts-new-exhibit-for-davis-centennial/',\n",
       " 'https://theaggie.org/2017/02/14/student-health-and-counseling-services-hosts-step-up-to-the-plate-campaign/',\n",
       " 'https://theaggie.org/2017/02/13/pe-classes-may-charge-additional-fees/',\n",
       " 'https://theaggie.org/2017/02/12/11-new-chancellor-fellows-honored-for-2016/',\n",
       " 'https://theaggie.org/2017/02/12/muslim-students-respond-to-recent-political-events/',\n",
       " 'https://theaggie.org/2017/02/12/sexcessful-campaign-launched-in-time-for-valentines-day/',\n",
       " 'https://theaggie.org/2017/02/10/michael-chan-sworn-in-as-interim-senator/',\n",
       " 'https://theaggie.org/2017/02/09/university-of-california-regents-meet-approve-first-tuition-raise-in-six-years/',\n",
       " 'https://theaggie.org/2017/02/09/last-week-in-senate-5/',\n",
       " 'https://theaggie.org/2017/02/09/uc-davis-receives-2-2-million-from-assembly-bill-2664/',\n",
       " 'https://theaggie.org/2017/02/06/senator-bill-dodd-visits-uc-davis/',\n",
       " 'https://theaggie.org/2017/02/05/ab-1887-prevents-use-of-state-funds-including-uc-funds-for-travel-to-states-with-anti-lgbt-laws/',\n",
       " 'https://theaggie.org/2017/02/02/uc-system-hires-title-ix-coordinator/',\n",
       " 'https://theaggie.org/2017/02/02/last-week-in-senate-4/',\n",
       " 'https://theaggie.org/2017/02/02/action-jackson-famous-breeding-donkey-dies-at-29/',\n",
       " 'https://theaggie.org/2017/02/02/asucd-senator-sam-park-resigns/',\n",
       " 'https://theaggie.org/2017/01/31/uc-davis-graduate-creates-online-website-to-connect-students-with-professionals-for-career-advice/',\n",
       " 'https://theaggie.org/2017/01/30/uc-davis-plans-to-bottle-student-wine-for-80/',\n",
       " 'https://theaggie.org/2017/01/30/uc-davis-to-host-first-ever-mental-health-conference/',\n",
       " 'https://theaggie.org/2017/01/29/uc-davis-sexual-violence-awareness-and-education-campaign-wins-award/',\n",
       " 'https://theaggie.org/2017/01/29/uc-davis-study-abroad-secures-22000-in-grants-from-french-government/',\n",
       " 'https://theaggie.org/2017/01/27/uc-regents-vote-to-raise-tuition-for-uc-campuses/',\n",
       " 'https://theaggie.org/2017/01/26/former-white-house-speechwriter-comes-to-uc-davis/',\n",
       " 'https://theaggie.org/2017/01/26/last-week-in-senate-3/',\n",
       " 'https://theaggie.org/2017/01/26/davis-area-receives-nearly-eight-inches-of-rain-in-first-half-of-january/',\n",
       " 'https://theaggie.org/2017/01/25/students-walk-out-of-classes-to-protest-trumps-climate-denial/',\n",
       " 'https://theaggie.org/2017/01/24/uc-davis-ranked-most-sustainable-university-in-the-world/',\n",
       " 'https://theaggie.org/2017/01/24/us-department-of-transportation-awards-14-million-grant-to-national-center-for-sustainable-transportation/',\n",
       " 'https://theaggie.org/2017/01/23/napolitano-to-reinforce-tentative-plans-to-expand-uc-davis-into-sacramento/',\n",
       " 'https://theaggie.org/2017/01/23/former-chancellor-turns-down-feminist-leadership-role-at-uc-davis/',\n",
       " 'https://theaggie.org/2017/01/22/sierra-nevada-brewing-owners-gift-2-million-to-uc-davis-program/',\n",
       " 'https://theaggie.org/2017/01/22/interim-chancellor-hexter-uc-leaders-sign-letter-to-urge-action-against-climate-change/',\n",
       " 'https://theaggie.org/2017/01/22/california-schools-press-donald-trump-to-continue-daca-as-president/',\n",
       " 'https://theaggie.org/2017/01/22/uc-wide-walkout-teach-ins-on-trumps-inauguration-day/',\n",
       " 'https://theaggie.org/2017/01/20/protests-erupt-at-milo-yiannopoulos-event/',\n",
       " 'https://theaggie.org/2017/01/20/student-organizers-host-changethe-conversation-event/',\n",
       " 'https://theaggie.org/2017/01/19/uc-president-janet-napolitano-hospitalized/',\n",
       " 'https://theaggie.org/2017/01/17/uc-davis-appoints-new-chief-of-nursing-and-patient-care-services/',\n",
       " 'https://theaggie.org/2017/01/16/student-regent-recruitment-for-the-2018-2019-school-year-begins/',\n",
       " 'https://theaggie.org/2017/01/15/russell-boulevard-intramural-fields-withdrawn-from-2017-2027-long-range-development-plan/',\n",
       " 'https://theaggie.org/2017/01/15/ucs-receive-record-breaking-number-of-applicants/',\n",
       " 'https://theaggie.org/2017/01/14/davis-college-republicans-club-leads-protest-against-cancellation-of-milo-yiannopoulos-event/',\n",
       " 'https://theaggie.org/2017/01/13/breaking-news-milo-yiannopoulos-event-cancelled/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_search('https://theaggie.org/campus', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_inf(url):\n",
    "    text = []\n",
    "    a_list = []\n",
    "    b_list = []\n",
    "    response = requests.get(url)\n",
    "    html_doc = response.text\n",
    "    soup = BeautifulSoup(html_doc, 'lxml')\n",
    "    result = soup.find_all('div', itemprop=\"articleBody\")\n",
    "    result_1 = result[0].find_all('p')\n",
    "    if \"\\n\" in result_1[-1]:\n",
    "        b_list.append((result_1[-1].text.split(\"\\n\"))[1])\n",
    "        author = b_list[0]\n",
    "    else:\n",
    "        author = result_1[-1].text  # author\n",
    "    del result_1[-1]\n",
    "    for i in result_1:\n",
    "        if \"\\n\" in result_1[-1]:\n",
    "            d = i.text.translate({ 0x2018:0x27, 0x2019:0x27, 0x201C:0x22, 0x201D:0x22, 0x2026:0x20 })## translate the unicode\n",
    "            a_list.append(d)\n",
    "            a_list = a_list + (result_1[-1].text.split(\"\\n\"))[0]\n",
    "        else:\n",
    "            d = i.text.translate({ 0x2018:0x27, 0x2019:0x27, 0x201C:0x22, 0x201D:0x22, 0x2026:0x20 })\n",
    "            a_list.append(d)\n",
    "    text = u' '.join(a_list)### text\n",
    "    title_1 = soup.find_all('h1', itemprop=\"headline\")# title \n",
    "    title = title_1[0].text\n",
    "    dict_1 = {\n",
    "            'author':author,\n",
    "            'text': text,\n",
    "            'title': title,\n",
    "            'url': url,\n",
    "              }\n",
    "           \n",
    "    return dict_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_inf('https://theaggie.org/2017/02/20/katehi-controversy-prompts-decline-of-uc-administrators-seeking-profitable-subsidiary-board-positions/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.2.__ Write a function that extracts the title, text, and author of an Aggie article. The function should:\n",
    "\n",
    "* Have a parameter `url` for the URL of the article.\n",
    "\n",
    "* For the author, extract the \"Written By\" line that appears at the end of most articles. You don't have to extract the author's name from this line.\n",
    "\n",
    "* Return a dictionary with keys \"url\", \"title\", \"text\", and \"author\". The values for these should be the article url, title, text, and author, respectively.\n",
    "\n",
    "For example, for [this article](https://theaggie.org/2017/02/14/project-toto-aims-to-address-questions-regarding-city-finances/) your function should return something similar to this:\n",
    "```\n",
    "{\n",
    "    'author': u'Written By: Bianca Antunez \\xa0\\u2014\\xa0city@theaggie.org',\n",
    "    'text': u'Davis residents create financial model to make city\\'s financial state more transparent To increase transparency between the city\\'s financial situation and the community, three residents created a model called Project Toto which aims to improve how the city communicates its finances in an easily accessible design. Jeff Miller and Matt Williams, who are members of Davis\\' Finance and Budget Commission, joined together with Davis entrepreneur Bob Fung to create the model plan to bring the project to the Finance and Budget Commission in February, according to Kelly Stachowicz, assistant city manager. \"City staff appreciate the efforts that have gone into this, and the interest in trying to look at the city\\'s potential financial position over the long term,\" Stachowicz said in an email interview. \"We all have a shared goal to plan for a sound fiscal future with few surprises. We believe the Project Toto effort will mesh well with our other efforts as we build the budget for the next fiscal year and beyond.\" Project Toto complements the city\\'s effort to amplify the transparency of city decisions to community members. The aim is to increase the understanding about the city\\'s financial situation and make the information more accessible and easier to understand. The project is mostly a tool for public education, but can also make predictions about potential decisions regarding the city\\'s financial future. Once completed, the program will allow residents to manipulate variables to see their eventual consequences, such as tax increases or extensions and proposed developments \"This really isn\\'t a budget, it is a forecast to see the intervention of these decisions,\" Williams said in an interview with The Davis Enterprise. \"What happens if we extend the sales tax? What does it do given the other numbers that are in?\" Project Toto enables users, whether it be a curious Davis resident, a concerned community member or a city leader, with the ability to project city finances with differing variables. The online program consists of the 400-page city budget for the 2016-2017 fiscal year, the previous budget, staff reports and consultant analyses. All of the documents are cited and accessible to the public within Project Toto. \"It\\'s a model that very easily lends itself to visual representation,\" Mayor Robb Davis said. \"You can see the impacts of decisions the council makes on the fiscal health of the city.\" Complementary to this program, there is also a more advanced version of the model with more in-depth analyses of the city\\'s finances. However, for an easy-to-understand, simplistic overview, Project Toto should be enough to help residents comprehend Davis finances. There is still more to do on the project, but its creators are hard at work trying to finalize it before the 2017-2018 fiscal year budget. \"It\\'s something I have been very much supportive of,\" Davis said. \"Transparency is not just something that I have been supportive of but something we have stated as a city council objective [ ] this fits very well with our attempt to inform the public of our challenges with our fiscal situation.\" ',\n",
    "    'title': 'Project Toto aims to address questions regarding city finances',\n",
    "    'url': 'https://theaggie.org/2017/02/14/project-toto-aims-to-address-questions-regarding-city-finances/'\n",
    "}\n",
    "```\n",
    "\n",
    "Hints:\n",
    "\n",
    "* The author line is always the last line of the last paragraph.\n",
    "\n",
    "*   Python 2 displays some Unicode characters as `\\uXXXX`. For instance, `\\u201c` is a left-facing quotation mark.\n",
    "    You can convert most of these to ASCII characters with the method call (on a string)\n",
    "    ```\n",
    "    .translate({ 0x2018:0x27, 0x2019:0x27, 0x201C:0x22, 0x201D:0x22, 0x2026:0x20 })\n",
    "    ```\n",
    "    If you're curious about these characters, you can look them up on [this page](http://unicode.org/cldr/utility/character.jsp), or read \n",
    "    more about [what Unicode is](http://unicode.org/standard/WhatIsUnicode.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.3.__ Use your functions from exercises 1.1 and 1.2 to get a data frame of 60 [Campus News](https://theaggie.org/campus/) articles and a data frame of 60 [City News](https://theaggie.org/city/) articles. Add a column to each that indicates the category, then combine them into one big data frame.\n",
    "\n",
    "The \"text\" column of this data frame will be your corpus for natural language processing in exercise 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_Campus = pd.DataFrame(extract_inf(i) for i in art_search('https://theaggie.org/campus', 4))\n",
    "result_Campus['category'] = 'Campus'\n",
    "result_City = pd.DataFrame(extract_inf(i) for i in art_search('https://theaggie.org/city', 4))\n",
    "result_City['category'] = 'City'\n",
    "result = pd.concat([result_Campus, result_City], axis = 0)\n",
    "result.index = [range(120)]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.4.__ Use the Aggie corpus to answer the following questions. Use plots to support your analysis.\n",
    "\n",
    "* What topics does the Aggie cover the most? Do city articles typically cover different topics than campus articles?\n",
    "\n",
    "* What are the titles of the top 3 pairs of most similar articles? Examine each pair of articles. What words do they have in common?\n",
    "\n",
    "* Do you think this corpus is representative of the Aggie? Why or why not? What kinds of inference can this corpus support? Explain your reasoning.\n",
    "\n",
    "Hints:\n",
    "\n",
    "*   The [nltk book](http://www.nltk.org/book/) and [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) may be helpful here.\n",
    "\n",
    "*   You can determine whether city articles are \"near\" campus articles from the similarity matrix or with k-nearest neighbors.\n",
    "\n",
    "*   If you want, you can use the [wordcloud](http://amueller.github.io/word_cloud/) package to plot a word cloud. To install the package, run\n",
    "    ```\n",
    "    conda install -c https://conda.anaconda.org/amueller wordcloud\n",
    "    ```\n",
    "    in a terminal. Word clouds look nice and are easy to read, but are less precise than bar plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2> Tokenize and Denoise </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = result['title']\n",
    "text = result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize = nltk.word_tokenize\n",
    "def stem(tokens,stemmer = PorterStemmer().stem):    \n",
    "    '''\n",
    "    Delete the word in text that is not needed, and transform it to lower case.\n",
    "    \n",
    "    Input: list of string \n",
    "    \n",
    "    Output: list of unicode\n",
    "    \n",
    "    '''\n",
    "    word_1 = []\n",
    "    del_words = stopwords.words('english')\n",
    "    del_words.extend([\"also\",\"Also\", \"one\", \",\", \"Go\", \"and\", \"in\", \".\", \"'s\", \"'\", \"'/'/\",\n",
    "                      \"The\", \"be\", \"to\", \"S\", \"said\", \"for\", \"is\", \"that\",\n",
    "                      \"uc\", \"as\", \"thi\", \"it\", \"with\",\"an\", \"at\", \"what\", \"It\"\n",
    "                      \"from\", \"not\", \"are\",\"had\", \"would\", \"could\", \"if\", \".\",\"it\"\n",
    "                      \"to\", \"those\",  \"since\", \"get\", \"as\", \"of\", \"''\", \"(\", \")\",\"[\", \"]\", '``',\n",
    "                      \"like\", \"one\", \"in\", \"!\" ,\"#\", \"%\", \"$\", \"&\", \"too\", \"go\",\"n't\",\"In\",\"I\",])\n",
    "    for words in tokens:\n",
    "        if words not in del_words:\n",
    "            word_1.append(words)      \n",
    "    return [stemmer(w.lower()) for w in word_1] \n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\"\n",
    "    Extract simple lemmas based on tokenization and stemming\n",
    "    Input: string\n",
    "    Output: list of strings (lemmata)\n",
    "    \"\"\"\n",
    "    return stem(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_fre(text):\n",
    "    '''\n",
    "    Count the word frequency\n",
    "    \n",
    "    Input: pd.Series of text\n",
    "    \n",
    "    Output: dictionary of unicode about the word frequency\n",
    "    \n",
    "    '''\n",
    "    textd = {} #dictionary from lemmata to document ids containing that lemma\n",
    "    for i in range(len(text)):\n",
    "        s = set(lemmatize(text.iloc[i]))\n",
    "        try:\n",
    "            toks = toks | s\n",
    "        except NameError: \n",
    "            toks = s\n",
    "        for tok in s:\n",
    "            try:\n",
    "                textd[tok].append(title.iloc[i])\n",
    "            except KeyError:\n",
    "                textd[tok] = [title.iloc[i]]\n",
    "    numd = {key:len(set(val)) for key,val in textd.items()}\n",
    "    return numd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> We define that if the word frequency is greater than 45, then it is called \"high frequency word\". The high frequency words denotes the different topics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numd = word_fre(text)\n",
    "numd = pd.DataFrame([numd])\n",
    "numd = numd.transpose()\n",
    "numd.columns = ['Frequency']\n",
    "numd = numd.sort_values(by='Frequency',ascending=False)\n",
    "numd_1 = numd[numd['Frequency'] >= 45]\n",
    "numd_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## High Frequency level Word\n",
    "numd_1.plot(y = 0, fontsize = 10, kind = 'barh', title = 'High Frequency Word')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.xlim(0,120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_city = []\n",
    "text_campus = []\n",
    "\n",
    "for txt in result_City[\"text\"]:\n",
    "    text_city.append(txt)\n",
    "    \n",
    "for txt in result_Campus[\"text\"]:\n",
    "    text_campus.append(txt)\n",
    "    \n",
    "text_campus_all = u' '.join(text_campus)\n",
    "text_city_all = u' '.join(text_city)\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmatize,stop_words=\"english\",smooth_idf=True,norm=\"l2\")\n",
    "total = [text_city_all,text_campus_all]\n",
    "tfs = vectorizer.fit_transform(total)\n",
    "sim_table = tfs.dot(tfs.T)\n",
    "print sim_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Strategy:             \n",
    "We want to find the high frequency word in campus article and city article seperately, and find the difference in topics between two types of articles.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Campus_text = result_Campus['text']\n",
    "City_text = result_City['text']\n",
    "numd_Campus = pd.DataFrame([word_fre(Campus_text)])\n",
    "numd_City = pd.DataFrame([word_fre(City_text)])\n",
    "numd_Campus = numd_Campus.transpose()\n",
    "numd_City = numd_City.transpose()\n",
    "numd_City.columns = ['Frequency']\n",
    "numd_Campus.columns = ['Frequency']\n",
    "numd_Campus = numd_Campus.sort_values(by='Frequency',ascending=False)\n",
    "numd_City = numd_City.sort_values(by='Frequency',ascending=False)\n",
    "\n",
    "numd_2 = numd_City[numd_City['Frequency'] >= 20]\n",
    "numd_3 = numd_Campus[numd_Campus['Frequency'] >= 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numd_2.plot(y = 0, fontsize = 10, kind = 'barh', title = 'High Frequency Word of City')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.xlim(0,120)\n",
    "plt.show()\n",
    "numd_3.plot(y = 0, fontsize = 10, kind = 'barh', title = 'High Frequency Word of Campus')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.xlim(0,120)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>\n",
    "<ul>\n",
    "<li>From the frequency graph, the topics that cover the most are \"ucdavis\", \"Student\", \"Major\", \"Work\",\"president\", \"California\".</li>\n",
    "<li>We can see that both type of article have the topics, such as \"community\", \"Davis\", \"work\", \"manipulate\", and\"help\".\n",
    "<li>From the similarity table, the similarity is 0.666. Therefore, campus article topics are quite similar to city article topics.</li>\n",
    "</ul>\n",
    "</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1.4.2       \n",
    "Vectorize and similar matrix </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2\n",
    "text_all = text_campus + text_city\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmatize,stop_words=\"english\",smooth_idf=True,norm=\"l2\")\n",
    "tfs_text = vectorizer.fit_transform(text_all)\n",
    "norm = tfs_text.dot(tfs_text.T)\n",
    "norm = norm.toarray()\n",
    "\n",
    "\n",
    "def sim_matrix(matrix):\n",
    "    '''\n",
    "    Argument: find three pairs of most similar articles.\n",
    "    \n",
    "    Input: matrix\n",
    "    \n",
    "    Output : the pairs of index of similar matrix.\n",
    "    '''\n",
    "    res_sort = []\n",
    "    sort_matrix = np.argsort(matrix,axis  = None)[::-1] # Transform the matrix to array and sort it \n",
    "    for i in sort_matrix:\n",
    "        if (i/120)!=(i%120):    # to remove the diagnole \n",
    "            res_sort.append(i)\n",
    "    result_sort_con  = [[res_sort[0]/120,res_sort[0]%120],[res_sort[2]/120,res_sort[2]%120],[res_sort[4]/120,res_sort[4]%120]]\n",
    "    return result_sort_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim_matrix(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print title.iloc[14]\n",
    "print title.iloc[35]\n",
    "print title.iloc[58]\n",
    "print title.iloc[51]\n",
    "print title.iloc[24]\n",
    "print title.iloc[38]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Words in common by using Word Clouds </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordcloud(text1, text2):\n",
    "    text_compare = text1 + text2\n",
    "    wc = WordCloud(background_color=\"white\", max_words=80)\n",
    "    wc.generate(text_compare)\n",
    "    return wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud(text.iloc[14], text.iloc[35]))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud(text.iloc[58], text.iloc[51]))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud(text.iloc[24], text.iloc[38]))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>\n",
    "Conclusion: \n",
    "<ul>\n",
    "<li>Between the article \"UC Davis holds first mental health conference\" and \"UC Davis to host first ever mental health conference\", there are some common words like \"mental\", \"student\", \"health\".</li>\n",
    "<li>Between the article \"Davis College Republicans club leads protest against cancellation of Milo Yiannopoulos event\" and \"Protests erupt at Milo Yiannopoulos event\", there are some common words like \"Milo Yiannopoulos\", \"protest\", \"event\".</li>\n",
    "<li>Between the article \"University of California Regents meet, approve first tuition raise in six years\" and \"UC Regents vote to raise tuition for UC campuses\", there are some common words, like\"tuision\", \"increase\", \"UC student\".</li>\n",
    "</ul>\n",
    "</strong>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1.4.3</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>\n",
    "The corpus can not be  representative of the Aggie.\n",
    "<ul>\n",
    "<li> First, there are only extracted from campus and city topic, so there are many topics it has never covered, such as sports and news.</li>\n",
    "<li>Second, All of the articles are from year 2017; Hence, it cannot represent the pass of Aggie.</li> \n",
    "</ul>\n",
    "</strong>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numd = word_fre(title)\n",
    "numd = pd.DataFrame([numd])\n",
    "numd = numd.transpose()\n",
    "numd.columns = ['Frequency']\n",
    "numd = numd.sort_values(by='Frequency',ascending=False)\n",
    "numd_1 = numd[numd['Frequency'] >= 5]\n",
    "numd_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>\n",
    "Inference:         \n",
    "According to the word frequency of the word in title , we can inference that recently the UCDavis are focusing on the protest of president election and the increasment of tuision.\n",
    "</strong>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
